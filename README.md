WavBench<div align="center">WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models[Paper]   [Website]   [Dataset]</div>Figure 1: Examples of Colloquial Expression in WavBench, covering diverse cognitive domains across Basic and Pro subsets.Figure 2: Examples of Acoustic Interaction in WavBench, demonstrating Explicit Understanding, Explicit Generation, and Implicit Dialogue.IntroductionWavBench is a comprehensive benchmark designed to evaluate the realistic conversational abilities of End-to-End (E2E) Spoken Dialogue Models. Unlike traditional benchmarks that focus primarily on ASR accuracy or simple text generation, WavBench establishes a tripartite framework to assess Cognitive Complexity, Colloquial Delivery, and Paralinguistic Fidelity.WavBench comprises 17,577 high-quality items totaling 76.5 hours, rigorously testing models across:Colloquial Expression: Assessing the model's ability to "speak" naturally rather than just "read" text, split into Pro (complex reasoning) and Basic (everyday chat) subsets.Acoustic Interaction: Evaluating the model's capability to perceive and generate fine-grained paralinguistic features (e.g., emotion, accent, background audio) through both Explicit instructions and Implicit dialogue.News2024.xx.xx Released the WavBench paper and evaluation toolkit.2024.xx.xx Released the WavBench Dataset on Hugging Face.Table of ContentsLeaderboardSetupDatasetEvaluationCitationLeaderboardBelow is the overall evaluation of state-of-the-art E2E spoken dialogue models on WavBench.Metrics / TasksQwen3-OmniKimi-AudioMimo-AudioStep-Audio-2GPT-4o AudioPanel A: Colloquial Expression (Pro)Code39.7530.2928.9631.2053.60Creativity48.3931.7842.8635.0063.00Instruction43.0129.8636.4429.4057.80Logic33.2126.0327.5726.2042.60Math38.5527.3025.6822.4050.20QA50.9342.5441.2840.8072.80Safety60.0056.1956.1952.4067.60Avg (Pro)39.5330.7932.0230.4058.23Panel B: Colloquial Expression (Basic)Code53.1040.6942.0737.2058.00Creativity57.4441.5745.2947.2071.20Instruction57.2944.4133.5636.6066.80Logic52.3550.7449.9148.8067.00Math51.0541.2738.7330.2062.40QA57.5449.0749.1248.6075.60Safety59.6758.8362.8360.2081.00Avg (Basic)55.8049.2349.5748.5068.80Panel C: Explicit UnderstandingAccent37.5011.0027.0020.6715.67Age64.3353.6753.0067.6720.33Emotion92.8677.3377.3375.4385.90Gender21.0044.5020.0068.0061.50Language83.5091.0053.5096.5097.00Pitch32.4423.1124.0034.2223.56Speed46.6754.6748.8944.0048.00Volume33.7838.2231.1150.6741.78Audio Event61.7367.9019.7539.5159.26Music22.2266.6755.5677.7833.33Avg (Understand)49.6052.8041.0257.3648.70Panel D: Explicit GenerationAccent37.503.5223.4422.0774.22Age64.6546.8851.9531.6478.12Emotion90.0450.2957.1366.5095.51Gender72.2745.3167.5859.7798.83Language89.8474.8051.5691.4187.89Pitch76.5647.2780.2755.6685.74Speed43.7547.2751.5669.1466.60Volume56.2564.0659.9657.0382.42Audio27.0310.819.4632.4345.95Music62.5020.8316.6770.8377.08Avg (Generation)62.0341.1046.9355.6579.23Panel E: Implicit InteractionSingle-Turn (Text)1.851.842.231.122.43Single-Turn (Audio)3.173.212.473.502.96Multi-Turn (Text)4.884.574.614.384.48Multi-Turn (Audio)1.251.081.041.211.23Avg (Implicit)2.782.672.592.552.78SetupBashconda create -n wavbench python=3.8+
conda activate wavbench
pip install -r requirements.txt
DatasetThe data used in this project is available at WavBench Dataset hosted on Hugging Face. The benchmark is structured into two primary categories.1. Colloquial ExpressionThis category evaluates the model's ability to maintain a natural, spoken style across various cognitive loads. It is divided into Basic (everyday interaction) and Pro (complex reasoning) subsets. Each subset covers 7 specific domains:Code: Explaining logic and algorithms conversationally.Creative Writing: Generating engaging content without rigid formatting.Instruction Following: Adhering to constraints suitable for speech.Logic: Solving puzzles and reasoning tasks naturally.Math: Verbalizing mathematical steps and proofs clearly.QA: Answering knowledge-based questions in a spoken format.Safety: Handling unsafe queries with appropriate spoken refusals.2. Acoustic InteractionThis category evaluates the model's paralinguistic capabilities across three dimensions:Explicit Understanding: The model must identify acoustic attributes from user input.Explicit Generation: The model must generate speech exhibiting specific acoustic attributes based on instructions.Attributes assessed (for both Understanding & Generation): Accent, Age, Emotion, Gender, Language, Pitch, Speed, Volume, Audio Event, Music.Implicit Interaction: The model must infer and generate appropriate acoustic styles from the dialogue context without explicit directives.Sub-tasks: Single-turn Text, Single-turn Audio, Multi-turn Text, Multi-turn Audio.EvaluationWavBench uses a unified pipeline: Inference $\rightarrow$ Evaluation $\rightarrow$ Statistics.Step 1: Run InferenceUse main.py to generate model responses. You can choose to output just text or both text and audio.Example: Run Colloquial Basic (with audio generation)Bashpython main.py --model step_audio2 --data basic_code --audio_output
Example: Run Acoustic Multi-round (with audio generation)Bashpython main.py --model step_audio2 --data acoustic_multi_round_generation --audio_output
Supported Arguments:--model: Model name (e.g., step_audio2).--data: Dataset name (e.g., basic_code, pro_math, acoustic_explicit_generation_emotion).--audio_output: Flag to enable audio generation (required for Acoustic tasks).Step 2: Automatic EvaluationUse evaluate.py to grade the responses. WavBench employs LLM-as-a-judge (using models like Gemini or GPT-4) to score responses based on "listenability," correctness, and paralinguistic fidelity.Evaluate a specific dataset:Bashexport GOOGLE_API_KEY="your-api-key"
python evaluate.py --eval_type colloquial --dataset basic_code
Evaluate all acoustic datasets:Bashpython evaluate.py --eval_type acoustic --dataset all
Supported Arguments:--eval_type: colloquial or acoustic.--dataset: Specific dataset name or all.--result_dir: Directory containing inference results (default: ./output).Step 3: Get StatisticsFinally, use statistics.py to aggregate the scores and generate a summary report.Bashpython statistics.py --eval_dir ./eval_results --output ./statistics.txt
CitationIf you use WavBench in your research, please cite the following paper:代码段@article{wavbench2024,
  title={WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models},
  author={WavBench Team},
  journal={arXiv preprint},
  year={2024}
}
